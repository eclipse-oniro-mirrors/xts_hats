/*
 * Copyright (c) 2024 Huawei Device Co., Ltd.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <vector>
#include <v2_0/nnrt_types.h>
#include <v2_0/innrt_device.h>
#include <v2_0/iprepared_model.h>

#include "gtest/gtest.h"
#include "mindir.h"
#include "mindir_lite_graph.h"

#include "interfaces/kits/c/neural_network_runtime/neural_network_runtime.h"
#include "common/hdi_nnrt_test_utils.h"
#include "common/hdi_nnrt_test.h"

using namespace std;
using namespace testing::ext;
using namespace OHOS::NeuralNetworkRuntime;
using namespace OHOS::NeuralNetworkRuntime::Test;

namespace {

class ModelPrepareTestAdditional : public HDINNRtTest {};

}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_0300
 * @tc.name   : testPrepareOfflineModel001
 * @tc.desc   : mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel001, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_0400
 * @tc.name   : testPrepareOfflineModel002
 * @tc.desc   : mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel002, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_0500
 * @tc.name   : testPrepareOfflineModel003
 * @tc.desc   : mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel003, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_0600
 * @tc.name   : testPrepareOfflineModel004
 * @tc.desc   : mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel004, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_0700
 * @tc.name   : testPrepareOfflineModel005
 * @tc.desc   : mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel005, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_0800
 * @tc.name   : testPrepareOfflineModel006
 * @tc.desc   : mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel006, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_0900
 * @tc.name   : testPrepareOfflineModel007
 * @tc.desc   : mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel007, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1000
 * @tc.name   : testPrepareOfflineModel008
 * @tc.desc   : mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel008, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1100
 * @tc.name   : testPrepareOfflineModel009
 * @tc.desc   : mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel009, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1200
 * @tc.name   : testPrepareOfflineModel010
 * @tc.desc   : mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel010, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1300
 * @tc.name   : testPrepareOfflineModel011
 * @tc.desc   : mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel011, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1400
 * @tc.name   : testPrepareOfflineModel012
 * @tc.desc   : mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel012, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1500
 * @tc.name   : testPrepareOfflineModel013
 * @tc.desc   : mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel013, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1600
 * @tc.name   : testPrepareOfflineModel014
 * @tc.desc   : mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel014, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1700
 * @tc.name   : testPrepareOfflineModel015
 * @tc.desc   : mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel015, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1800
 * @tc.name   : testPrepareOfflineModel016
 * @tc.desc   : mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel016, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_1900
 * @tc.name   : testPrepareOfflineModel017
 * @tc.desc   : mode = V2_0::PERFORMANCE_HIGH,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel017, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_HIGH;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2000
 * @tc.name   : testPrepareOfflineModel018
 * @tc.desc   : mode = V2_0::PERFORMANCE_HIGH,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel018, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_HIGH;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2100
 * @tc.name   : testPrepareOfflineModel019
 * @tc.desc   : mode = V2_0::PERFORMANCE_HIGH,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel019, Function | MediumTest | Level2)
{
    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_HIGH;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    std::vector<V2_0::SharedBuffer> modelCache;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2200
 * @tc.name   : testPrepareOfflineModel020
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel020, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2300
 * @tc.name   : testPrepareOfflineModel021
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel021, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2400
 * @tc.name   : testPrepareOfflineModel022
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel022, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2500
 * @tc.name   : testPrepareOfflineModel023
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_NONE,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel023, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2600
 * @tc.name   : testPrepareOfflineModel024
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel024, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2700
 * @tc.name   : testPrepareOfflineModel025
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel025, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2800
 * @tc.name   : testPrepareOfflineModel026
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel026, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_2900
 * @tc.name   : testPrepareOfflineModel027
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_LOW,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel027, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_LOW;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3000
 * @tc.name   : testPrepareOfflineModel028
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel028, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3100
 * @tc.name   : testPrepareOfflineModel029
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel029, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3200
 * @tc.name   : testPrepareOfflineModel030
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel030, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3300
 * @tc.name   : testPrepareOfflineModel031
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_MEDIUM,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel031, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_MEDIUM;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3400
 * @tc.name   : testPrepareOfflineModel032
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel032, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3500
 * @tc.name   : testPrepareOfflineModel033
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel033, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3600
 * @tc.name   : testPrepareOfflineModel034
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel034, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3700
 * @tc.name   : testPrepareOfflineModel035
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_EXTREME,priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel035, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_EXTREME;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3800
 * @tc.name   : testPrepareOfflineModel036
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_HIGH,priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel036, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_HIGH;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_3900
 * @tc.name   : testPrepareOfflineModel037
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_HIGH,priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel037, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_HIGH;
    modelConfig.priority = V2_0::PRIORITY_LOW;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_PrepareOfflineModel_4000
 * @tc.name   : testPrepareOfflineModel038
 * @tc.desc   : call ExportModelCache,mode = V2_0::PERFORMANCE_HIGH,priority = V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareOfflineModel038, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_HIGH;
    modelConfig.priority = V2_0::PRIORITY_MEDIUM;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_Run_0200
 * @tc.name   : testRun001
 * @tc.desc   : model run,cycle 10 times
 */
HWTEST_F(ModelPrepareTestAdditional, testRun001, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_HIGH;
    modelConfig.priority = V2_0::PRIORITY_HIGH;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModelFromOffline;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareOfflineModel(modelCache, modelConfig, iPreparedModelFromOffline));

    std::vector<V2_0::IOTensor> inputs;
    std::vector<V2_0::IOTensor> outputs;
    std::vector<std::vector<int32_t>> outputsDims;
    std::vector<void *> mapedMemorys;

    std::vector<float> inputValue = {ADD_VALUE_1, ADD_VALUE_2};
    for (uint32_t i = 0; i < inputValue.size(); ++i) {
        std::vector<float> data(ADDEND_DATA_SIZE, inputValue[i]);
        auto tensor = HDICommon::CreateIOTensor(device_);
        auto memAddress = HDICommon::MapMemory(tensor.data.fd, ADDEND_BUFFER_LENGTH);
        mapedMemorys.emplace_back(memAddress);
        HDICommon::SetData((float *)memAddress, ADDEND_BUFFER_LENGTH, (float *)data.data());
        inputs.emplace_back(tensor);
    }
    auto outputTensor = HDICommon::CreateIOTensor(device_);
    outputs.emplace_back(outputTensor);
    for (int a = 0; a < 10; a++) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModelFromOffline->Run(inputs, outputs, outputsDims));
    }

    auto memAddress = HDICommon::MapMemory(outputs[0].data.fd, ADDEND_BUFFER_LENGTH);
    mapedMemorys.emplace_back(memAddress);

    auto buffer = (float *)memAddress;
    std::vector<float> expectValue(ADDEND_DATA_SIZE, ADD_VALUE_RESULT);
    std::vector<float> outputValue(buffer, buffer + ADDEND_DATA_SIZE);
    EXPECT_TRUE(CheckExpectOutput(outputValue, expectValue)) << "output value check failed.";

    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
    HDICommon::ReleaseBufferOfTensors(device_, inputs);
    HDICommon::ReleaseBufferOfTensors(device_, outputs);
    HDICommon::UnmapAllMemory(mapedMemorys);
}

/**
 * @tc.number : SUB_AI_NNRt_Func_South_Model_GetInputDimRanges_0300
 * @tc.name   : testGetInputDimRanges001
 * @tc.desc   : [C- SOFTWARE -0200],cycle 10 times
 */
HWTEST_F(ModelPrepareTestAdditional, testGetInputDimRanges001, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraphDynamic(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig;
    modelConfig.enableFloat16 = false;
    modelConfig.mode = V2_0::PERFORMANCE_NONE;
    modelConfig.priority = V2_0::PRIORITY_NONE;

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, iPreparedModel));

    std::vector<std::vector<uint32_t>> minInputsDim;
    std::vector<std::vector<uint32_t>> maxInputsDim;
    for (int a = 0; a < 10; a++) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->GetInputDimRanges(minInputsDim, maxInputsDim));
    }
    printf("the value of minInputsDim : ");
    for (size_t i = 0; i < minInputsDim.size(); ++i) {
        for (size_t j = 0; j < minInputsDim.size(); ++j) {
            printf("%u ", minInputsDim[i][j]);
        }
    }
    printf("\n");

    printf("the value of maxInputsDim : ");
    for (size_t i = 0; i < maxInputsDim.size(); ++i) {
        for (size_t j = 0; j < maxInputsDim.size(); ++j) {
            printf("%u ", maxInputsDim[i][j]);
        }
    }
    printf("\n");

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_ExportModelCacheV2_0200
 * @tc.name   : testExportModelCache002
 * @tc.desc   : Call ExportModelCache 100 times to test stability
 */
HWTEST_F(ModelPrepareTestAdditional, testExportModelCache002, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;

    for (int i = 0; i < 100; i++) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0100
 * @tc.name: testPrepareModel001
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_NONE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel001, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0200
 * @tc.name: testPrepareModel002
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_ACTIVATION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel002, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0300
 * @tc.name: testPrepareModel003
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_ADD_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel003, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0400
 * @tc.name: testPrepareModel004
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_ARGMAX_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel004, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0500
 * @tc.name: testPrepareModel005
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_AVG_POOL_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel005, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0600
 * @tc.name: testPrepareModel006
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_BATCH_TO_SPACE_ND
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel006, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0700
 * @tc.name: testPrepareModel007
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_BIAS_ADD
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel007, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0800
 * @tc.name: testPrepareModel008
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_CAST
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel008, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0900
 * @tc.name: testPrepareModel009
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_CONCAT
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel009, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1000
 * @tc.name: testPrepareModel010
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_CONV2D_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel010, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1100
 * @tc.name: testPrepareModel011
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel011, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1200
 * @tc.name: testPrepareModel012
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_DIV_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel012, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1300
 * @tc.name: testPrepareModel013
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_ELTWISE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel013, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1400
 * @tc.name: testPrepareModel014
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_EXPAND_DIMS
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel014, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1500
 * @tc.name: testPrepareModel015
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_FILL
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel015, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1600
 * @tc.name: testPrepareModel016
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_FULL_CONNECTION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel016, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1700
 * @tc.name: testPrepareModel017
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_FUSED_BATCH_NORM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel017, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1800
 * @tc.name: testPrepareModel018
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_GATHER
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel018, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1900
 * @tc.name: testPrepareModel019
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_LAYER_NORM_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel019, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2000
 * @tc.name: testPrepareModel020
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_LESS_EQUAL
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel020, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2100
 * @tc.name: testPrepareModel021
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_MATMUL_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel021, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2200
 * @tc.name: testPrepareModel022
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_MAXIMUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel022, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2300
 * @tc.name: testPrepareModel023
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_MAX_POOL_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel023, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2400
 * @tc.name: testPrepareModel024
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_MUL_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel024, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2500
 * @tc.name: testPrepareModel025
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_ONE_HOT
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel025, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2600
 * @tc.name: testPrepareModel026
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_PAD_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel026, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2700
 * @tc.name: testPrepareModel027
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_POW_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel027, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2800
 * @tc.name: testPrepareModel028
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_PRELU_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel028, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2900
 * @tc.name: testPrepareModel029
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_QUANT_DTYPE_CAST
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel029, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3000
 * @tc.name: testPrepareModel030
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_REDUCE_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel030, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3100
 * @tc.name: testPrepareModel031
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_RESHAPE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel031, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3200
 * @tc.name: testPrepareModel032
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_RESIZE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel032, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3300
 * @tc.name: testPrepareModel033
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_RSQRT
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel033, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3400
 * @tc.name: testPrepareModel034
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SCALE_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel034, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3500
 * @tc.name: testPrepareModel035
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SHAPE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel035, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3600
 * @tc.name: testPrepareModel036
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SLICE_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel036, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3700
 * @tc.name: testPrepareModel037
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SOFTMAX
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel037, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3800
 * @tc.name: testPrepareModel038
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SPACE_TO_BATCH_ND
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel038, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3900
 * @tc.name: testPrepareModel039
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SPLIT
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel039, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4000
 * @tc.name: testPrepareModel040
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SQRT
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel040, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4100
 * @tc.name: testPrepareModel041
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SQUEEZE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel041, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4200
 * @tc.name: testPrepareModel042
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SQUARED_DIFFERENCE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel042, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4300
 * @tc.name: testPrepareModel043
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_STACK
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel043, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4400
 * @tc.name: testPrepareModel044
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_STRIDED_SLICE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel044, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4500
 * @tc.name: testPrepareModel045
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_SUB_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel045, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4600
 * @tc.name: testPrepareModel046
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_TILE_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel046, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4700
 * @tc.name: testPrepareModel047
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_TOPK_FUSION
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel047, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4800
 * @tc.name: testPrepareModel048
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_TRANSPOSE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel048, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4900
 * @tc.name: testPrepareModel049
 * @tc.desc: Call function V2 PrepareModel, node.nodeType is NODE_TYPE_UNSQUEEZE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel049, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5000
 * @tc.name: testPrepareModel050
 * @tc.desc: Call function V2 PrepareModel, node.quantType is QUANT_TYPE_NONE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel050, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.quantType = static_cast<V2_0::QuantType>(mindspore::lite::QUANT_TYPE_NONE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5100
 * @tc.name: testPrepareModel051
 * @tc.desc: Call function V2 PrepareModel, node.quantType is QUANT_TYPE_ALL
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel051, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.quantType = static_cast<V2_0::QuantType>(mindspore::lite::QUANT_TYPE_ALL);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5200
 * @tc.name: testPrepareModel052
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_UNKNOWN
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel052, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.dataType = static_cast<V2_0::DataType>(mindspore::lite::DATA_TYPE_UNKNOWN);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5300
 * @tc.name: testPrepareModel053
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_BOOL
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel053, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.dataType = static_cast<V2_0::DataType>(mindspore::lite::DATA_TYPE_BOOL);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5400
 * @tc.name: testPrepareModel054
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_INT8
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel054, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.dataType = static_cast<V2_0::DataType>(mindspore::lite::DATA_TYPE_INT8);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5500
 * @tc.name: testPrepareModel055
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_INT16
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel055, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}
/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5600
 * @tc.name: testPrepareModel056
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_INT32
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel056, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.dataType = static_cast<V2_0::DataType>(mindspore::lite::DATA_TYPE_INT32);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5700
 * @tc.name: testPrepareModel057
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_INT64
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel057, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5800
 * @tc.name: testPrepareModel058
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_UINT8
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel058, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5900
 * @tc.name: testPrepareModel059
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_UINT16
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel059, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6000
 * @tc.name: testPrepareModel060
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_UINT32
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel060, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6100
 * @tc.name: testPrepareModel061
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_UINT64
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel061, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6200
 * @tc.name: testPrepareModel062
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_FLOAT16
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel062, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.dataType = static_cast<V2_0::DataType>(mindspore::lite::DATA_TYPE_FLOAT16);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6300
 * @tc.name: testPrepareModel063
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_FLOAT32
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel063, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.dataType = static_cast<V2_0::DataType>(mindspore::lite::DATA_TYPE_FLOAT32);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6400
 * @tc.name: testPrepareModel064
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is DATA_TYPE_FLOAT64
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel064, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6500
 * @tc.name: testPrepareModel065
 * @tc.desc: Call function V2 PrepareModel, tensor.format is FORMAT_NONE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel065, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.format = static_cast<V2_0::Format>(OHOS::HDI::Nnrt::V2_0::FORMAT_NONE);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6600
 * @tc.name: testPrepareModel066
 * @tc.desc: Call function V2 PrepareModel, tensor.format is FORMAT_NCHW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel066, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.format = static_cast<V2_0::Format>(mindspore::lite::FORMAT_NCHW);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6700
 * @tc.name: testPrepareModel067
 * @tc.desc: Call function V2 PrepareModel, tensor.format is FORMAT_NHWC
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel067, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.format = static_cast<V2_0::Format>(mindspore::lite::FORMAT_NHWC);
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6800
 * @tc.name: testPrepareModel068
 * @tc.desc: Call function V2 PrepareModel, node.quantType is 1000
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel068, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &node = iModel->nodes[0];
    node.quantType = static_cast<V2_0::QuantType>(1000);

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};

    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6900
 * @tc.name: testPrepareModel069
 * @tc.desc: Call function V2 PrepareModel, node.quantType is -1
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel069, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &node = iModel->nodes[0];
    node.quantType = static_cast<V2_0::QuantType>(-1);

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};

    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7000
 * @tc.name: testPrepareModel070
 * @tc.desc: Call function V2 PrepareModel, tensor.dataType is -1
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel070, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = static_cast<V2_0::DataType>(-1);

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};

    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_DATATYPE, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7100
 * @tc.name: testPrepareModel071
 * @tc.desc: Call function V2 PrepareModel, Tensor.format is -1
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel071, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.format = static_cast<V2_0::Format>(-1);

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};

    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7200
 * @tc.name: testPrepareModel072
 * @tc.desc: Call function V2 PrepareModel, tensor.dims is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel072, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.dims = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7300
 * @tc.name: testPrepareModel073
 * @tc.desc: Call function V2 PrepareModel, tensor.data is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel073, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.data = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_MEMORY_ERROR, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7400
 * @tc.name: testPrepareModel074
 * @tc.desc: Call function V2 PrepareModel, tensor.quantParams is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel074, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &tensor : iModel->allTensors) {
        tensor.quantParams = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7500
 * @tc.name: testPrepareModel075
 * @tc.desc: Call function V2 PrepareModel, node.nodeAttr is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel075, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeAttr = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7600
 * @tc.name: testPrepareModel076
 * @tc.desc: Call function V2 PrepareModel, node.inputIndex is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel076, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.inputIndex = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7700
 * @tc.name: testPrepareModel077
 * @tc.desc: Call function V2 PrepareModel, node.outputIndex is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel077, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.outputIndex = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7800
 * @tc.name: testPrepareModel078
 * @tc.desc: Call function V2 PrepareModel, subgraph.inputIndices is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel078, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &subgraph : iModel->subGraph) {
        subgraph.inputIndices = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7900
 * @tc.name: testPrepareModel079
 * @tc.desc: Call function V2 PrepareModel, subgraph.outputIndices is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel079, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &subgraph : iModel->subGraph) {
        subgraph.outputIndices = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8000
 * @tc.name: testPrepareModel080
 * @tc.desc: Call function V2 PrepareModel, subgraph.nodeIndices is null
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel080, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &subgraph : iModel->subGraph) {
        subgraph.nodeIndices = {};
    }

    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8100
 * @tc.name: testPrepareModel081
 * @tc.desc: Call function V2 PrepareModel, mode is PERFORMANCE_NONE
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel081, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_NONE),
                                  V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8200
 * @tc.name: testPrepareModel082
 * @tc.desc: Call function V2 PrepareModel, mode is PERFORMANCE_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel082, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_LOW), V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8300
 * @tc.name: testPrepareModel083
 * @tc.desc: Call function V2 PrepareModel, mode is PERFORMANCE_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel083, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_MEDIUM),
                                  V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8400
 * @tc.name: testPrepareModel084
 * @tc.desc: Call function V2 PrepareModel, mode is PERFORMANCE_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel084, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_HIGH),
                                  V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8500
 * @tc.name: testPrepareModel085
 * @tc.desc: Call function V2 PrepareModel, mode is PERFORMANCE_EXTREME
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel085, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_EXTREME),
                                  V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8600
 * @tc.name: testPrepareModel086
 * @tc.desc: Call function V2 PrepareModel, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel086, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_NONE), V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8700
 * @tc.name: testPrepareModel087
 * @tc.desc: Call function V2 PrepareModel, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel087, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_NONE),
                                  V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number: SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8800
 * @tc.name: testPrepareModel088
 * @tc.desc: Call function V2 PrepareModel, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel088, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));
    V2_0::ModelConfig modelConfig{true, static_cast<V2_0::PerformanceMode>(V2_0::PERFORMANCE_NONE),
                                  V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8900
 * @tc.name   : testPrepareModel089
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel089, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9000
 * @tc.name   : testPrepareModel090
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel090, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9100
 * @tc.name   : testPrepareModel091
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel091, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9200
 * @tc.name   : testPrepareModel092
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel092, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9300
 * @tc.name   : testPrepareModel093
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel093, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9400
 * @tc.name   : testPrepareModel094
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel094, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9500
 * @tc.name   : testPrepareModel095
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel095, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9600
 * @tc.name   : testPrepareModel096
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel096, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9700
 * @tc.name   : testPrepareModel097
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel097, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9800
 * @tc.name   : testPrepareModel098
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel098, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_9900
 * @tc.name   : testPrepareModel099
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel099, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0010
 * @tc.name   : testPrepareModel100
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel100, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0020
 * @tc.name   : testPrepareModel101
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel101, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0030
 * @tc.name   : testPrepareModel102
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel102, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0040
 * @tc.name   : testPrepareModel103
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel103, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0050
 * @tc.name   : testPrepareModel104
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel104, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0060
 * @tc.name   : testPrepareModel105
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * NODE_TYPE_FUSED_BATCH_NORM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel105, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0070
 * @tc.name   : testPrepareModel106
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel106, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0080
 * @tc.name   : testPrepareModel107
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel107, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0090
 * @tc.name   : testPrepareModel108
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel108, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0100
 * @tc.name   : testPrepareModel109
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel109, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0110
 * @tc.name   : testPrepareModel110
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel110, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0120
 * @tc.name   : testPrepareModel111
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel111, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0130
 * @tc.name   : testPrepareModel112
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel112, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0140
 * @tc.name   : testPrepareModel113
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel113, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0150
 * @tc.name   : testPrepareModel114
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel114, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0160
 * @tc.name   : testPrepareModel115
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel115, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0170
 * @tc.name   : testPrepareModel116
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel116, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0180
 * @tc.name   : testPrepareModel117
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel117, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0190
 * @tc.name   : testPrepareModel118
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel118, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0200
 * @tc.name   : testPrepareModel119
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel119, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0210
 * @tc.name   : testPrepareModel120
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel120, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0220
 * @tc.name   : testPrepareModel121
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel121, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0230
 * @tc.name   : testPrepareModel122
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel122, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0240
 * @tc.name   : testPrepareModel123
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel123, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0250
 * @tc.name   : testPrepareModel124
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel124, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0260
 * @tc.name   : testPrepareModel125
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel125, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0270
 * @tc.name   : testPrepareModel126
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel126, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0280
 * @tc.name   : testPrepareModel127
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel127, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0290
 * @tc.name   : testPrepareModel128
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel128, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0300
 * @tc.name   : testPrepareModel129
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel129, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0310
 * @tc.name   : testPrepareModel130
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel130, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0320
 * @tc.name   : testPrepareModel131
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel131, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0330
 * @tc.name   : testPrepareModel132
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel132, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0340
 * @tc.name   : testPrepareModel133
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel133, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0350
 * @tc.name   : testPrepareModel134
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel134, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0360
 * @tc.name   : testPrepareModel135
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel135, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0370
 * @tc.name   : testPrepareModel136
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel136, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0380
 * @tc.name   : testPrepareModel137
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel137, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0390
 * @tc.name   : testPrepareModel138
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel138, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0400
 * @tc.name   : testPrepareModel139
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel139, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0410
 * @tc.name   : testPrepareModel140
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel140, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0420
 * @tc.name   : testPrepareModel141
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel141, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0430
 * @tc.name   : testPrepareModel142
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel142, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0440
 * @tc.name   : testPrepareModel143
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel143, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0450
 * @tc.name   : testPrepareModel144
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel144, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0460
 * @tc.name   : testPrepareModel145
 * @tc.desc   : Call function V2 PrepareModel, NodeType is PERFORMANCE_MEDIUM, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel145, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0470
 * @tc.name   : testPrepareModel146
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel146, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0480
 * @tc.name   : testPrepareModel147
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel147, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0490
 * @tc.name   : testPrepareModel148
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel148, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0500
 * @tc.name   : testPrepareModel149
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel149, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0510
 * @tc.name   : testPrepareModel150
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel150, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0520
 * @tc.name   : testPrepareModel151
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel151, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0530
 * @tc.name   : testPrepareModel152
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel152, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0540
 * @tc.name   : testPrepareModel153
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel153, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0550
 * @tc.name   : testPrepareModel154
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel154, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0560
 * @tc.name   : testPrepareModel155
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel155, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0570
 * @tc.name   : testPrepareModel156
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel156, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0580
 * @tc.name   : testPrepareModel157
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel157, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0590
 * @tc.name   : testPrepareModel158
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel158, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0600
 * @tc.name   : testPrepareModel159
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel159, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0610
 * @tc.name   : testPrepareModel160
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel160, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0620
 * @tc.name   : testPrepareModel161
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel161, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0630
 * @tc.name   : testPrepareModel162
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel162, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0640
 * @tc.name   : testPrepareModel163
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel163, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0650
 * @tc.name   : testPrepareModel164
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel164, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0660
 * @tc.name   : testPrepareModel165
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel165, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0670
 * @tc.name   : testPrepareModel166
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel166, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0680
 * @tc.name   : testPrepareModel167
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel167, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0690
 * @tc.name   : testPrepareModel168
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel168, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0700
 * @tc.name   : testPrepareModel169
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel169, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0710
 * @tc.name   : testPrepareModel170
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel170, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0720
 * @tc.name   : testPrepareModel171
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel171, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0730
 * @tc.name   : testPrepareModel172
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel172, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0740
 * @tc.name   : testPrepareModel173
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel173, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0750
 * @tc.name   : testPrepareModel174
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel174, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0760
 * @tc.name   : testPrepareModel175
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel175, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0770
 * @tc.name   : testPrepareModel176
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel176, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0780
 * @tc.name   : testPrepareModel177
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel177, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0790
 * @tc.name   : testPrepareModel178
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel178, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0800
 * @tc.name   : testPrepareModel179
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel179, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0810
 * @tc.name   : testPrepareModel180
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel180, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0820
 * @tc.name   : testPrepareModel181
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel181, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0830
 * @tc.name   : testPrepareModel182
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel182, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0840
 * @tc.name   : testPrepareModel183
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel183, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0850
 * @tc.name   : testPrepareModel184
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel184, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0860
 * @tc.name   : testPrepareModel185
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel185, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0870
 * @tc.name   : testPrepareModel186
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel186, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0880
 * @tc.name   : testPrepareModel187
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel187, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0890
 * @tc.name   : testPrepareModel188
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel188, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0900
 * @tc.name   : testPrepareModel189
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel189, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0910
 * @tc.name   : testPrepareModel190
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel190, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0920
 * @tc.name   : testPrepareModel191
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel191, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0930
 * @tc.name   : testPrepareModel192
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel192, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0940
 * @tc.name   : testPrepareModel193
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel193, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0950
 * @tc.name   : testPrepareModel194
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel194, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0960
 * @tc.name   : testPrepareModel195
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel195, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0970
 * @tc.name   : testPrepareModel196
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel196, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0980
 * @tc.name   : testPrepareModel197
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel197, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_0990
 * @tc.name   : testPrepareModel198
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel198, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1000
 * @tc.name   : testPrepareModel199
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel199, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1010
 * @tc.name   : testPrepareModel200
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel200, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1020
 * @tc.name   : testPrepareModel201
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel201, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1030
 * @tc.name   : testPrepareModel202
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel202, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1040
 * @tc.name   : testPrepareModel203
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel203, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1050
 * @tc.name   : testPrepareModel204
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel204, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1060
 * @tc.name   : testPrepareModel205
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel205, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1070
 * @tc.name   : testPrepareModel206
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel206, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1080
 * @tc.name   : testPrepareModel207
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel207, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1090
 * @tc.name   : testPrepareModel208
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel208, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1100
 * @tc.name   : testPrepareModel209
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel209, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1110
 * @tc.name   : testPrepareModel210
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel210, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1120
 * @tc.name   : testPrepareModel211
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel211, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1130
 * @tc.name   : testPrepareModel212
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel212, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1140
 * @tc.name   : testPrepareModel213
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel213, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1150
 * @tc.name   : testPrepareModel214
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel214, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1160
 * @tc.name   : testPrepareModel215
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel215, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1170
 * @tc.name   : testPrepareModel216
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel216, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1180
 * @tc.name   : testPrepareModel217
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel217, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1190
 * @tc.name   : testPrepareModel218
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel218, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1200
 * @tc.name   : testPrepareModel219
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel219, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1210
 * @tc.name   : testPrepareModel220
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel220, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1220
 * @tc.name   : testPrepareModel221
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel221, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1230
 * @tc.name   : testPrepareModel222
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel222, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1240
 * @tc.name   : testPrepareModel223
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel223, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1250
 * @tc.name   : testPrepareModel224
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel224, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1260
 * @tc.name   : testPrepareModel225
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel225, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1270
 * @tc.name   : testPrepareModel226
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel226, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1280
 * @tc.name   : testPrepareModel227
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel227, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1290
 * @tc.name   : testPrepareModel228
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel228, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1300
 * @tc.name   : testPrepareModel229
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel229, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1310
 * @tc.name   : testPrepareModel230
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel230, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1320
 * @tc.name   : testPrepareModel231
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel231, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1330
 * @tc.name   : testPrepareModel232
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel232, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1340
 * @tc.name   : testPrepareModel233
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel233, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1350
 * @tc.name   : testPrepareModel234
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel234, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1360
 * @tc.name   : testPrepareModel235
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel235, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1370
 * @tc.name   : testPrepareModel236
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel236, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1380
 * @tc.name   : testPrepareModel237
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel237, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1390
 * @tc.name   : testPrepareModel238
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel238, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1400
 * @tc.name   : testPrepareModel239
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel239, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1410
 * @tc.name   : testPrepareModel240
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel240, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1420
 * @tc.name   : testPrepareModel241
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel241, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1430
 * @tc.name   : testPrepareModel242
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel242, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1440
 * @tc.name   : testPrepareModel243
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel243, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1450
 * @tc.name   : testPrepareModel244
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel244, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1460
 * @tc.name   : testPrepareModel245
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel245, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1470
 * @tc.name   : testPrepareModel246
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel246, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1480
 * @tc.name   : testPrepareModel247
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel247, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1490
 * @tc.name   : testPrepareModel248
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel248, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1500
 * @tc.name   : testPrepareModel249
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel249, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1510
 * @tc.name   : testPrepareModel250
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel250, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1520
 * @tc.name   : testPrepareModel251
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel251, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1530
 * @tc.name   : testPrepareModel252
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel252, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1540
 * @tc.name   : testPrepareModel253
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel253, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1550
 * @tc.name   : testPrepareModel254
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel254, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1560
 * @tc.name   : testPrepareModel255
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel255, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1570
 * @tc.name   : testPrepareModel256
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel256, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1580
 * @tc.name   : testPrepareModel257
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel257, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1590
 * @tc.name   : testPrepareModel258
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel258, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1600
 * @tc.name   : testPrepareModel259
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel259, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1610
 * @tc.name   : testPrepareModel260
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel260, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1620
 * @tc.name   : testPrepareModel261
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel261, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1630
 * @tc.name   : testPrepareModel262
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel262, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1640
 * @tc.name   : testPrepareModel263
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel263, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1650
 * @tc.name   : testPrepareModel264
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel264, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1660
 * @tc.name   : testPrepareModel265
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel265, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1670
 * @tc.name   : testPrepareModel266
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel266, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1680
 * @tc.name   : testPrepareModel267
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel267, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1690
 * @tc.name   : testPrepareModel268
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel268, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1700
 * @tc.name   : testPrepareModel269
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel269, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1710
 * @tc.name   : testPrepareModel270
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel270, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1720
 * @tc.name   : testPrepareModel271
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel271, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1730
 * @tc.name   : testPrepareModel272
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel272, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1740
 * @tc.name   : testPrepareModel273
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel273, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1750
 * @tc.name   : testPrepareModel274
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel274, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1760
 * @tc.name   : testPrepareModel275
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel275, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1770
 * @tc.name   : testPrepareModel276
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel276, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1780
 * @tc.name   : testPrepareModel277
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel277, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1790
 * @tc.name   : testPrepareModel278
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel278, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1800
 * @tc.name   : testPrepareModel279
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel279, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1810
 * @tc.name   : testPrepareModel280
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel280, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1820
 * @tc.name   : testPrepareModel281
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel281, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1830
 * @tc.name   : testPrepareModel282
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel282, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1840
 * @tc.name   : testPrepareModel283
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel283, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1850
 * @tc.name   : testPrepareModel284
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_LOW
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel284, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1860
 * @tc.name   : testPrepareModel285
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel285, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1870
 * @tc.name   : testPrepareModel286
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel286, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1880
 * @tc.name   : testPrepareModel287
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel287, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1890
 * @tc.name   : testPrepareModel288
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel288, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1900
 * @tc.name   : testPrepareModel289
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel289, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1910
 * @tc.name   : testPrepareModel290
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel290, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1920
 * @tc.name   : testPrepareModel291
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel291, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1930
 * @tc.name   : testPrepareModel292
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel292, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1940
 * @tc.name   : testPrepareModel293
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel293, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1950
 * @tc.name   : testPrepareModel294
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel294, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1960
 * @tc.name   : testPrepareModel295
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel295, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1970
 * @tc.name   : testPrepareModel296
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel296, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1980
 * @tc.name   : testPrepareModel297
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel297, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_1990
 * @tc.name   : testPrepareModel298
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel298, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2000
 * @tc.name   : testPrepareModel299
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel299, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2010
 * @tc.name   : testPrepareModel300
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel300, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2020
 * @tc.name   : testPrepareModel301
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel301, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2030
 * @tc.name   : testPrepareModel302
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel302, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2040
 * @tc.name   : testPrepareModel303
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel303, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2050
 * @tc.name   : testPrepareModel304
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel304, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2060
 * @tc.name   : testPrepareModel305
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel305, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2070
 * @tc.name   : testPrepareModel306
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel306, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2080
 * @tc.name   : testPrepareModel307
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel307, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2090
 * @tc.name   : testPrepareModel308
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel308, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2100
 * @tc.name   : testPrepareModel309
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel309, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2110
 * @tc.name   : testPrepareModel310
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel310, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2120
 * @tc.name   : testPrepareModel311
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel311, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2130
 * @tc.name   : testPrepareModel312
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel312, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2140
 * @tc.name   : testPrepareModel313
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel313, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2150
 * @tc.name   : testPrepareModel314
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel314, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2160
 * @tc.name   : testPrepareModel315
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel315, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2170
 * @tc.name   : testPrepareModel316
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel316, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2180
 * @tc.name   : testPrepareModel317
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel317, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2190
 * @tc.name   : testPrepareModel318
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel318, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2200
 * @tc.name   : testPrepareModel319
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel319, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2210
 * @tc.name   : testPrepareModel320
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel320, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2220
 * @tc.name   : testPrepareModel321
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel321, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2230
 * @tc.name   : testPrepareModel322
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel322, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2240
 * @tc.name   : testPrepareModel323
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel323, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2250
 * @tc.name   : testPrepareModel324
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel324, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2260
 * @tc.name   : testPrepareModel325
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel325, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2270
 * @tc.name   : testPrepareModel326
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel326, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2280
 * @tc.name   : testPrepareModel327
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel327, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2290
 * @tc.name   : testPrepareModel328
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel328, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2300
 * @tc.name   : testPrepareModel329
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel329, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2310
 * @tc.name   : testPrepareModel330
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel330, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2320
 * @tc.name   : testPrepareModel331
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel331, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2330
 * @tc.name   : testPrepareModel332
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel332, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2340
 * @tc.name   : testPrepareModel333
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel333, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2350
 * @tc.name   : testPrepareModel334
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel334, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2360
 * @tc.name   : testPrepareModel335
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel335, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2370
 * @tc.name   : testPrepareModel336
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel336, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2380
 * @tc.name   : testPrepareModel337
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel337, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2390
 * @tc.name   : testPrepareModel338
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel338, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2400
 * @tc.name   : testPrepareModel339
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel339, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2410
 * @tc.name   : testPrepareModel340
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel340, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2420
 * @tc.name   : testPrepareModel341
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel341, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2430
 * @tc.name   : testPrepareModel342
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel342, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2440
 * @tc.name   : testPrepareModel343
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel343, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2450
 * @tc.name   : testPrepareModel344
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel344, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2460
 * @tc.name   : testPrepareModel345
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel345, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2470
 * @tc.name   : testPrepareModel346
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel346, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2480
 * @tc.name   : testPrepareModel347
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel347, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2490
 * @tc.name   : testPrepareModel348
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel348, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2500
 * @tc.name   : testPrepareModel349
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel349, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2510
 * @tc.name   : testPrepareModel350
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel350, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2520
 * @tc.name   : testPrepareModel351
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel351, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2530
 * @tc.name   : testPrepareModel352
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel352, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2540
 * @tc.name   : testPrepareModel353
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel353, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2550
 * @tc.name   : testPrepareModel354
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel354, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2560
 * @tc.name   : testPrepareModel355
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel355, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2570
 * @tc.name   : testPrepareModel356
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel356, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2580
 * @tc.name   : testPrepareModel357
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel357, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2590
 * @tc.name   : testPrepareModel358
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel358, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2600
 * @tc.name   : testPrepareModel359
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel359, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2610
 * @tc.name   : testPrepareModel360
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel360, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2620
 * @tc.name   : testPrepareModel361
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel361, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2630
 * @tc.name   : testPrepareModel362
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel362, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2640
 * @tc.name   : testPrepareModel363
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel363, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2650
 * @tc.name   : testPrepareModel364
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel364, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2660
 * @tc.name   : testPrepareModel365
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel365, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2670
 * @tc.name   : testPrepareModel366
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel366, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2680
 * @tc.name   : testPrepareModel367
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel367, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2690
 * @tc.name   : testPrepareModel368
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel368, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2700
 * @tc.name   : testPrepareModel369
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel369, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2710
 * @tc.name   : testPrepareModel370
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel370, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2720
 * @tc.name   : testPrepareModel371
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel371, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2730
 * @tc.name   : testPrepareModel372
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel372, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2740
 * @tc.name   : testPrepareModel373
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel373, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2750
 * @tc.name   : testPrepareModel374
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel374, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2760
 * @tc.name   : testPrepareModel375
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel375, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2770
 * @tc.name   : testPrepareModel376
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel376, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2780
 * @tc.name   : testPrepareModel377
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel377, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2790
 * @tc.name   : testPrepareModel378
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel378, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2800
 * @tc.name   : testPrepareModel379
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel379, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2810
 * @tc.name   : testPrepareModel380
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel380, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2820
 * @tc.name   : testPrepareModel381
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel381, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2830
 * @tc.name   : testPrepareModel382
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel382, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2840
 * @tc.name   : testPrepareModel383
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel383, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2850
 * @tc.name   : testPrepareModel384
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel384, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2860
 * @tc.name   : testPrepareModel385
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel385, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2870
 * @tc.name   : testPrepareModel386
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel386, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2880
 * @tc.name   : testPrepareModel387
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel387, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2890
 * @tc.name   : testPrepareModel388
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel388, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2900
 * @tc.name   : testPrepareModel389
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel389, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2910
 * @tc.name   : testPrepareModel390
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel390, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2920
 * @tc.name   : testPrepareModel391
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel391, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2930
 * @tc.name   : testPrepareModel392
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel392, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2940
 * @tc.name   : testPrepareModel393
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel393, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2950
 * @tc.name   : testPrepareModel394
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel394, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2960
 * @tc.name   : testPrepareModel395
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel395, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2970
 * @tc.name   : testPrepareModel396
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel396, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2980
 * @tc.name   : testPrepareModel397
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel397, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_2990
 * @tc.name   : testPrepareModel398
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel398, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3000
 * @tc.name   : testPrepareModel399
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel399, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3010
 * @tc.name   : testPrepareModel400
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel400, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3020
 * @tc.name   : testPrepareModel401
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel401, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3030
 * @tc.name   : testPrepareModel402
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel402, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3040
 * @tc.name   : testPrepareModel403
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel403, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3050
 * @tc.name   : testPrepareModel404
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel404, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3060
 * @tc.name   : testPrepareModel405
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel405, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3070
 * @tc.name   : testPrepareModel406
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel406, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3080
 * @tc.name   : testPrepareModel407
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel407, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3090
 * @tc.name   : testPrepareModel408
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel408, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3100
 * @tc.name   : testPrepareModel409
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel409, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3110
 * @tc.name   : testPrepareModel410
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel410, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3120
 * @tc.name   : testPrepareModel411
 * @tc.desc   : VCall function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel411, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3130
 * @tc.name   : testPrepareModel412
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel412, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3140
 * @tc.name   : testPrepareModel413
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel413, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3150
 * @tc.name   : testPrepareModel414
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel414, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3160
 * @tc.name   : testPrepareModel415
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel415, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3170
 * @tc.name   : testPrepareModel416
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel416, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3180
 * @tc.name   : testPrepareModel417
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel417, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3190
 * @tc.name   : testPrepareModel418
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel418, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3200
 * @tc.name   : testPrepareModel419
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel419, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3210
 * @tc.name   : testPrepareModel420
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel420, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3220
 * @tc.name   : testPrepareModel421
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel421, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3230
 * @tc.name   : testPrepareModel422
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NNODE_TYPE_SQRTONE, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel422, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3240
 * @tc.name   : testPrepareModel423
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel423, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3250
 * @tc.name   : testPrepareModel424
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel424, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3260
 * @tc.name   : testPrepareModel425
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel425, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3270
 * @tc.name   : testPrepareModel426
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel426, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3280
 * @tc.name   : testPrepareModel427
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel427, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3290
 * @tc.name   : testPrepareModel428
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel428, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3300
 * @tc.name   : testPrepareModel429
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel429, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3310
 * @tc.name   : testPrepareModel430
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel430, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3320
 * @tc.name   : testPrepareModel431
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel431, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3330
 * @tc.name   : testPrepareModel432
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel432, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3340
 * @tc.name   : testPrepareModel433
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel433, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3350
 * @tc.name   : testPrepareModel434
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel434, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3360
 * @tc.name   : testPrepareModel435
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel435, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3370
 * @tc.name   : testPrepareModel436
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel436, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3380
 * @tc.name   : testPrepareModel437
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel437, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3390
 * @tc.name   : testPrepareModel438
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel438, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3400
 * @tc.name   : testPrepareModel439
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel439, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3410
 * @tc.name   : testPrepareModel440
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel440, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3420
 * @tc.name   : testPrepareModel441
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel441, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3430
 * @tc.name   : testPrepareModel442
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel442, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3440
 * @tc.name   : testPrepareModel443
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel443, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3450
 * @tc.name   : testPrepareModel444
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel444, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3460
 * @tc.name   : testPrepareModel445
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel445, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3470
 * @tc.name   : testPrepareModel446
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel446, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3480
 * @tc.name   : testPrepareModel447
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel447, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3490
 * @tc.name   : testPrepareModel448
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel448, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3500
 * @tc.name   : testPrepareModel449
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel449, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3510
 * @tc.name   : testPrepareModel450
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel450, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3520
 * @tc.name   : testPrepareModel451
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel451, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3530
 * @tc.name   : testPrepareModel452
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel452, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3540
 * @tc.name   : testPrepareModel453
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel453, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3550
 * @tc.name   : testPrepareModel454
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel454, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3560
 * @tc.name   : testPrepareModel455
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel455, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3570
 * @tc.name   : testPrepareModel456
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel456, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3580
 * @tc.name   : testPrepareModel457
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel457, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3590
 * @tc.name   : testPrepareModel458
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel458, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3600
 * @tc.name   : testPrepareModel459
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel459, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3610
 * @tc.name   : testPrepareModel460
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel460, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3620
 * @tc.name   : testPrepareModel461
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel461, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3630
 * @tc.name   : testPrepareModel462
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel462, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3640
 * @tc.name   : testPrepareModel463
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel463, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3650
 * @tc.name   : testPrepareModel464
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel464, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3660
 * @tc.name   : testPrepareModel465
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel465, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3670
 * @tc.name   : testPrepareModel466
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel466, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3680
 * @tc.name   : testPrepareModel467
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel467, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3690
 * @tc.name   : testPrepareModel468
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel468, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3700
 * @tc.name   : testPrepareModel469
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel469, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3710
 * @tc.name   : testPrepareModel470
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel470, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3720
 * @tc.name   : testPrepareModel471
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel471, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3730
 * @tc.name   : testPrepareModel472
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel472, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3740
 * @tc.name   : testPrepareModel473
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel473, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3750
 * @tc.name   : testPrepareModel474
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel474, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3760
 * @tc.name   : testPrepareModel475
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel475, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3770
 * @tc.name   : testPrepareModel476
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel476, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3780
 * @tc.name   : testPrepareModel477
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel477, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3790
 * @tc.name   : testPrepareModel478
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel478, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3800
 * @tc.name   : testPrepareModel479
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel479, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3810
 * @tc.name   : testPrepareModel480
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_MEDIUM
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel480, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3820
 * @tc.name   : testPrepareModel481
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel481, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3830
 * @tc.name   : testPrepareModel482
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel482, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3840
 * @tc.name   : testPrepareModel483
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel483, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3850
 * @tc.name   : testPrepareModel484
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel484, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3860
 * @tc.name   : testPrepareModel485
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel485, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3870
 * @tc.name   : testPrepareModel486
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel486, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3880
 * @tc.name   : testPrepareModel487
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel487, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3890
 * @tc.name   : testPrepareModel488
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel488, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3900
 * @tc.name   : testPrepareModel489
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel489, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3910
 * @tc.name   : testPrepareModel490
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSIONNODE_TYPE_CONV2D_FUSION,
 * PerformanceMode is PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel490, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3920
 * @tc.name   : testPrepareModel491
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel491, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3930
 * @tc.name   : testPrepareModel492
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel492, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3940
 * @tc.name   : testPrepareModel493
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel493, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3950
 * @tc.name   : testPrepareModel494
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel494, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3960
 * @tc.name   : testPrepareModel495
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel495, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3970
 * @tc.name   : testPrepareModel496
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel496, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3980
 * @tc.name   : testPrepareModel497
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel497, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_3990
 * @tc.name   : testPrepareModel498
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel498, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4000
 * @tc.name   : testPrepareModel499
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel499, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4010
 * @tc.name   : testPrepareModel500
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel500, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4020
 * @tc.name   : testPrepareModel501
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel501, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4030
 * @tc.name   : testPrepareModel502
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel502, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4040
 * @tc.name   : testPrepareModel503
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel503, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4050
 * @tc.name   : testPrepareModel504
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel504, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4060
 * @tc.name   : testPrepareModel505
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel505, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4070
 * @tc.name   : testPrepareModel506
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel506, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4080
 * @tc.name   : testPrepareModel507
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel507, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4090
 * @tc.name   : testPrepareModel508
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel508, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4100
 * @tc.name   : testPrepareModel509
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel509, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4110
 * @tc.name   : testPrepareModel510
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel510, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4120
 * @tc.name   : testPrepareModel511
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel511, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4130
 * @tc.name   : testPrepareModel512
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel512, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4140
 * @tc.name   : testPrepareModel513
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel513, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4150
 * @tc.name   : testPrepareModel514
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel514, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4160
 * @tc.name   : testPrepareModel515
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel515, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4170
 * @tc.name   : testPrepareModel516
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel516, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4180
 * @tc.name   : testPrepareModel517
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel517, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4190
 * @tc.name   : testPrepareModel518
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel518, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4200
 * @tc.name   : testPrepareModel519
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel519, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4210
 * @tc.name   : testPrepareModel520
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel520, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4220
 * @tc.name   : testPrepareModel521
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel521, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4230
 * @tc.name   : testPrepareModel522
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_LOW, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel522, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4240
 * @tc.name   : testPrepareModel523
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_LOW, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel523, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4250
 * @tc.name   : testPrepareModel524
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel524, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4260
 * @tc.name   : testPrepareModel525
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel525, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4270
 * @tc.name   : testPrepareModel526
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel526, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4280
 * @tc.name   : testPrepareModel527
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel527, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4290
 * @tc.name   : testPrepareModel528
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel528, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4300
 * @tc.name   : testPrepareModel529
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_LOW,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel529, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4310
 * @tc.name   : testPrepareModel530
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel530, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4320
 * @tc.name   : testPrepareModel531
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel531, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4330
 * @tc.name   : testPrepareModel532
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel532, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4340
 * @tc.name   : testPrepareModel533
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel533, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4350
 * @tc.name   : testPrepareModel534
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel534, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4360
 * @tc.name   : testPrepareModel535
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel535, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4370
 * @tc.name   : testPrepareModel536
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel536, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4380
 * @tc.name   : testPrepareModel537
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel537, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4390
 * @tc.name   : testPrepareModel538
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel538, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4400
 * @tc.name   : testPrepareModel539
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel539, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4410
 * @tc.name   : testPrepareModel540
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel540, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4420
 * @tc.name   : testPrepareModel541
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel541, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4430
 * @tc.name   : testPrepareModel542
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel542, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4440
 * @tc.name   : testPrepareModel543
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel543, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4450
 * @tc.name   : testPrepareModel544
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel544, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4460
 * @tc.name   : testPrepareModel545
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel545, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4470
 * @tc.name   : testPrepareModel546
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel546, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4480
 * @tc.name   : testPrepareModel547
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel547, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4490
 * @tc.name   : testPrepareModel548
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel548, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4500
 * @tc.name   : testPrepareModel549
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel549, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4510
 * @tc.name   : testPrepareModel550
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel550, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4520
 * @tc.name   : testPrepareModel551
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel551, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4530
 * @tc.name   : testPrepareModel552
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel552, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4540
 * @tc.name   : testPrepareModel553
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel553, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4550
 * @tc.name   : testPrepareModel554
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel554, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4560
 * @tc.name   : testPrepareModel555
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel555, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4570
 * @tc.name   : testPrepareModel556
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel556, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4580
 * @tc.name   : testPrepareModel557
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel557, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4590
 * @tc.name   : testPrepareModel558
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel558, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4600
 * @tc.name   : testPrepareModel559
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel559, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4610
 * @tc.name   : testPrepareModel560
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel560, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4620
 * @tc.name   : testPrepareModel561
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel561, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4630
 * @tc.name   : testPrepareModel562
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel562, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4640
 * @tc.name   : testPrepareModel563
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel563, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4650
 * @tc.name   : testPrepareModel564
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel564, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4660
 * @tc.name   : testPrepareModel565
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel565, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4670
 * @tc.name   : testPrepareModel566
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel566, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4680
 * @tc.name   : testPrepareModel567
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel567, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4690
 * @tc.name   : testPrepareModel568
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel568, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4700
 * @tc.name   : testPrepareModel569
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel569, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4710
 * @tc.name   : testPrepareModel570
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel570, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4720
 * @tc.name   : testPrepareModel571
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel571, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4730
 * @tc.name   : testPrepareModel572
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel572, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4740
 * @tc.name   : testPrepareModel573
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is
 * PERFORMANCE_MEDIUM, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel573, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4750
 * @tc.name   : testPrepareModel574
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel574, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4760
 * @tc.name   : testPrepareModel575
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel575, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4770
 * @tc.name   : testPrepareModel576
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel576, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4780
 * @tc.name   : testPrepareModel577
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel577, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4790
 * @tc.name   : testPrepareModel578
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_MEDIUM,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel578, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4800
 * @tc.name   : testPrepareModel579
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel579, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4810
 * @tc.name   : testPrepareModel580
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel580, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4820
 * @tc.name   : testPrepareModel581
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel581, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4830
 * @tc.name   : testPrepareModel582
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel582, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4840
 * @tc.name   : testPrepareModel583
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel583, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4850
 * @tc.name   : testPrepareModel584
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel584, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4860
 * @tc.name   : testPrepareModel585
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel585, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4870
 * @tc.name   : testPrepareModel586
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel586, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4880
 * @tc.name   : testPrepareModel587
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel587, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4890
 * @tc.name   : testPrepareModel588
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel588, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4900
 * @tc.name   : testPrepareModel589
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel589, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4910
 * @tc.name   : testPrepareModel590
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel590, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4920
 * @tc.name   : testPrepareModel591
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel591, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4930
 * @tc.name   : testPrepareModel592
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel592, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4940
 * @tc.name   : testPrepareModel593
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel593, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4950
 * @tc.name   : testPrepareModel594
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel594, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4960
 * @tc.name   : testPrepareModel595
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel595, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4970
 * @tc.name   : testPrepareModel596
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel596, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4980
 * @tc.name   : testPrepareModel597
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel597, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_4990
 * @tc.name   : testPrepareModel598
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel598, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5000
 * @tc.name   : testPrepareModel599
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel599, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5010
 * @tc.name   : testPrepareModel600
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel600, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5020
 * @tc.name   : testPrepareModel601
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel601, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5030
 * @tc.name   : testPrepareModel602
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel602, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5040
 * @tc.name   : testPrepareModel603
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel603, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5050
 * @tc.name   : testPrepareModel604
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel604, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5060
 * @tc.name   : testPrepareModel605
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel605, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5070
 * @tc.name   : testPrepareModel606
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel606, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5080
 * @tc.name   : testPrepareModel607
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel607, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5090
 * @tc.name   : testPrepareModel608
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel608, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5100
 * @tc.name   : testPrepareModel609
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel609, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5110
 * @tc.name   : testPrepareModel610
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel610, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5120
 * @tc.name   : testPrepareModel611
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel611, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5130
 * @tc.name   : testPrepareModel612
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel612, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5140
 * @tc.name   : testPrepareModel613
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel613, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5150
 * @tc.name   : testPrepareModel614
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel614, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5160
 * @tc.name   : testPrepareModel615
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel615, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5170
 * @tc.name   : testPrepareModel616
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel616, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5180
 * @tc.name   : testPrepareModel617
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel617, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5190
 * @tc.name   : testPrepareModel618
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel618, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5200
 * @tc.name   : testPrepareModel619
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel619, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5210
 * @tc.name   : testPrepareModel620
 * @tc.desc   : VCall function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_HIGH, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel620, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5220
 * @tc.name   : testPrepareModel621
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel621, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5230
 * @tc.name   : testPrepareModel622
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel622, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5240
 * @tc.name   : testPrepareModel623
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel623, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5250
 * @tc.name   : testPrepareModel624
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel624, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5260
 * @tc.name   : testPrepareModel625
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel625, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5270
 * @tc.name   : testPrepareModel626
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_HIGH, priority
 * is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel626, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5280
 * @tc.name   : testPrepareModel627
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_HIGH,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel627, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5290
 * @tc.name   : testPrepareModel628
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_NONE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel628, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_NONE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5300
 * @tc.name   : testPrepareModel629
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ACTIVATION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel629, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ACTIVATION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5310
 * @tc.name   : testPrepareModel630
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ADD_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel630, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ADD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5320
 * @tc.name   : testPrepareModel631
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ARGMAX_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel631, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ARGMAX_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5330
 * @tc.name   : testPrepareModel632
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_AVG_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel632, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_AVG_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5340
 * @tc.name   : testPrepareModel633
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BATCH_TO_SPACE_ND, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel633, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BATCH_TO_SPACE_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5350
 * @tc.name   : testPrepareModel634
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_BIAS_ADD, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel634, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_BIAS_ADD);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5360
 * @tc.name   : testPrepareModel635
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CAST, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel635, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5370
 * @tc.name   : testPrepareModel636
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONCAT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel636, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONCAT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5380
 * @tc.name   : testPrepareModel637
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel637, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5390
 * @tc.name   : testPrepareModel638
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_CONV2D_TRANSPOSE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel638, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_CONV2D_TRANSPOSE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5400
 * @tc.name   : testPrepareModel639
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_DIV_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel639, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_DIV_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5410
 * @tc.name   : testPrepareModel640
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ELTWISE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel640, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ELTWISE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5420
 * @tc.name   : testPrepareModel641
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_EXPAND_DIMS, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel641, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_EXPAND_DIMS);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5430
 * @tc.name   : testPrepareModel642
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FILL, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel642, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FILL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5440
 * @tc.name   : testPrepareModel643
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FULL_CONNECTION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel643, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FULL_CONNECTION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5450
 * @tc.name   : testPrepareModel644
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_FUSED_BATCH_NORM, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel644, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_FUSED_BATCH_NORM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5460
 * @tc.name   : testPrepareModel645
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_GATHER, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel645, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_GATHER);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5470
 * @tc.name   : testPrepareModel646
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LAYER_NORM_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel646, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LAYER_NORM_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5480
 * @tc.name   : testPrepareModel647
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_LESS_EQUAL, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel647, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_LESS_EQUAL);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5490
 * @tc.name   : testPrepareModel648
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MATMUL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel648, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MATMUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5500
 * @tc.name   : testPrepareModel649
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAXIMUM, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel649, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAXIMUM);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5510
 * @tc.name   : testPrepareModel650
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MAX_POOL_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel650, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MAX_POOL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5520
 * @tc.name   : testPrepareModel651
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_MUL_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel651, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_MUL_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5530
 * @tc.name   : testPrepareModel652
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_ONE_HOT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel652, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_ONE_HOT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5540
 * @tc.name   : testPrepareModel653
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PAD_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel653, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PAD_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5550
 * @tc.name   : testPrepareModel654
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_POW_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel654, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_POW_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5560
 * @tc.name   : testPrepareModel655
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_PRELU_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel655, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_PRELU_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5570
 * @tc.name   : testPrepareModel656
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_QUANT_DTYPE_CAST, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel656, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_QUANT_DTYPE_CAST);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5580
 * @tc.name   : testPrepareModel657
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_REDUCE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel657, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_REDUCE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5590
 * @tc.name   : testPrepareModel658
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESHAPE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel658, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5600
 * @tc.name   : testPrepareModel659
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RESIZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel659, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RESIZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5610
 * @tc.name   : testPrepareModel660
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_RSQRT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel660, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_RSQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5620
 * @tc.name   : testPrepareModel661
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SCALE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel661, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SCALE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5630
 * @tc.name   : testPrepareModel662
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SHAPE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel662, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SHAPE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5640
 * @tc.name   : testPrepareModel663
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SLICE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel663, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SLICE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5650
 * @tc.name   : testPrepareModel664
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SOFTMAX, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel664, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SOFTMAX);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5660
 * @tc.name   : testPrepareModel665
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPACE_TO_BATCH_ND, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel665, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPACE_TO_BATCH_ND);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5670
 * @tc.name   : testPrepareModel666
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SPLIT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel666, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SPLIT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5680
 * @tc.name   : testPrepareModel667
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQRT, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel667, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQRT);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5690
 * @tc.name   : testPrepareModel668
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUEEZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel668, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5700
 * @tc.name   : testPrepareModel669
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SQUARED_DIFFERENCE, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel669, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SQUARED_DIFFERENCE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5710
 * @tc.name   : testPrepareModel670
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STACK, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel670, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STACK);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5720
 * @tc.name   : testPrepareModel671
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_STRIDED_SLICE, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel671, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_STRIDED_SLICE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5730
 * @tc.name   : testPrepareModel672
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_SUB_FUSION, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel672, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_SUB_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5740
 * @tc.name   : testPrepareModel673
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TILE_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel673, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TILE_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5750
 * @tc.name   : testPrepareModel674
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TOPK_FUSION, PerformanceMode is
 * PERFORMANCE_EXTREME, priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel674, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TOPK_FUSION);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5760
 * @tc.name   : testPrepareModel675
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_TRANSPOSE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel675, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_TRANSPOSE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5770
 * @tc.name   : testPrepareModel676
 * @tc.desc   : Call function V2 PrepareModel, NodeType is NODE_TYPE_UNSQUEEZE, PerformanceMode is PERFORMANCE_EXTREME,
 * priority is PRIORITY_HIGH
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel676, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    for (auto &node : iModel->nodes) {
        node.nodeType = static_cast<V2_0::NodeType>(mindspore::lite::NODE_TYPE_UNSQUEEZE);
    }
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    std::vector<bool> supportedOperations;
    bool isAllSupported = true;

    device_->GetSupportedOperation(*iModel, supportedOperations);
    for (uint32_t i = 0; i < supportedOperations.size(); i++) {
        if (supportedOperations[i] == false) {
            isAllSupported = false;
        }
    }

    if (isAllSupported == true) {
        int32_t ret = device_->PrepareModel(*iModel, modelConfig, preparedModel);
        EXPECT_TRUE((ret == V2_0::NNRT_ReturnCode::NNRT_SUCCESS) || (ret == V2_0::NNRT_ReturnCode::NNRT_INVALID_NODE));
    }

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5780
 * @tc.name   : testPrepareModel677
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel677, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5790
 * @tc.name   : testPrepareModel678
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel678, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5800
 * @tc.name   : testPrepareModel679
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel679, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5810
 * @tc.name   : testPrepareModel680
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel680, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5820
 * @tc.name   : testPrepareModel681
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel681, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5830
 * @tc.name   : testPrepareModel682
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel682, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5840
 * @tc.name   : testPrepareModel683
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel683, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5850
 * @tc.name   : testPrepareModel684
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel684, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5860
 * @tc.name   : testPrepareModel685
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel685, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5870
 * @tc.name   : testPrepareModel686
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel686, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5880
 * @tc.name   : testPrepareModel687
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel687, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5890
 * @tc.name   : testPrepareModel688
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel688, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5900
 * @tc.name   : testPrepareModel689
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel689, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5910
 * @tc.name   : testPrepareModel690
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel690, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5920
 * @tc.name   : testPrepareModel691
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel691, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5930
 * @tc.name   : testPrepareModel692
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel692, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5940
 * @tc.name   : testPrepareModel693
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel693, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5950
 * @tc.name   : testPrepareModel694
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel694, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5960
 * @tc.name   : testPrepareModel695
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel695, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5970
 * @tc.name   : testPrepareModel696
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel696, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UNKNOWN;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5980
 * @tc.name   : testPrepareModel697
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel697, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_5990
 * @tc.name   : testPrepareModel698
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel698, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6000
 * @tc.name   : testPrepareModel699
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel699, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6010
 * @tc.name   : testPrepareModel700
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel700, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6020
 * @tc.name   : testPrepareModel701
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel701, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6030
 * @tc.name   : testPrepareModel702
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel702, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6040
 * @tc.name   : testPrepareModel703
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel703, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6050
 * @tc.name   : testPrepareModel704
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel704, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6060
 * @tc.name   : testPrepareModel705
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel705, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6070
 * @tc.name   : testPrepareModel706
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel706, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6080
 * @tc.name   : testPrepareModel707
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel707, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6090
 * @tc.name   : testPrepareModel708
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel708, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6100
 * @tc.name   : testPrepareModel709
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel709, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6110
 * @tc.name   : testPrepareModel710
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel710, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6120
 * @tc.name   : testPrepareModel711
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel711, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6130
 * @tc.name   : testPrepareModel712
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel712, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6140
 * @tc.name   : testPrepareModel713
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel713, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6150
 * @tc.name   : testPrepareModel714
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel714, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6160
 * @tc.name   : testPrepareModel715
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel715, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6170
 * @tc.name   : testPrepareModel716
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel716, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_BOOL;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6180
 * @tc.name   : testPrepareModel717
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel717, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6190
 * @tc.name   : testPrepareModel718
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel718, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6200
 * @tc.name   : testPrepareModel719
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel719, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6210
 * @tc.name   : testPrepareModel720
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel720, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6220
 * @tc.name   : testPrepareModel721
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel721, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6230
 * @tc.name   : testPrepareModel722
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel722, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6240
 * @tc.name   : testPrepareModel723
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel723, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6250
 * @tc.name   : testPrepareModel724
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel724, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6260
 * @tc.name   : testPrepareModel725
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel725, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6270
 * @tc.name   : testPrepareModel726
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel726, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6280
 * @tc.name   : testPrepareModel727
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel727, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6290
 * @tc.name   : testPrepareModel728
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel728, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6300
 * @tc.name   : testPrepareModel729
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel729, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6310
 * @tc.name   : testPrepareModel730
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel730, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6320
 * @tc.name   : testPrepareModel731
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel731, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6330
 * @tc.name   : testPrepareModel732
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel732, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6340
 * @tc.name   : testPrepareModel733
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel733, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6350
 * @tc.name   : testPrepareModel734
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel734, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6360
 * @tc.name   : testPrepareModel735
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel735, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6370
 * @tc.name   : testPrepareModel736
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel736, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6380
 * @tc.name   : testPrepareModel737
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel737, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6390
 * @tc.name   : testPrepareModel738
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel738, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6400
 * @tc.name   : testPrepareModel739
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel739, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6410
 * @tc.name   : testPrepareModel740
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel740, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6420
 * @tc.name   : testPrepareModel741
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel741, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6430
 * @tc.name   : testPrepareModel742
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel742, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6440
 * @tc.name   : testPrepareModel743
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel743, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6450
 * @tc.name   : testPrepareModel744
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel744, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6460
 * @tc.name   : testPrepareModel745
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel745, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6470
 * @tc.name   : testPrepareModel746
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel746, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6480
 * @tc.name   : testPrepareModel747
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel747, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6490
 * @tc.name   : testPrepareModel748
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel748, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6500
 * @tc.name   : testPrepareModel749
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel749, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6510
 * @tc.name   : testPrepareModel750
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel750, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6520
 * @tc.name   : testPrepareModel751
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel751, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6530
 * @tc.name   : testPrepareModel752
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel752, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6540
 * @tc.name   : testPrepareModel753
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel753, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6550
 * @tc.name   : testPrepareModel754
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel754, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6560
 * @tc.name   : testPrepareModel755
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel755, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6570
 * @tc.name   : testPrepareModel756
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel756, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6580
 * @tc.name   : testPrepareModel757
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel757, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6590
 * @tc.name   : testPrepareModel758
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel758, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6600
 * @tc.name   : testPrepareModel759
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel759, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6610
 * @tc.name   : testPrepareModel760
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel760, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6620
 * @tc.name   : testPrepareModel761
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel761, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6630
 * @tc.name   : testPrepareModel762
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel762, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6640
 * @tc.name   : testPrepareModel763
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel763, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6650
 * @tc.name   : testPrepareModel764
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel764, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6660
 * @tc.name   : testPrepareModel765
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel765, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6670
 * @tc.name   : testPrepareModel766
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel766, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6680
 * @tc.name   : testPrepareModel767
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel767, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6690
 * @tc.name   : testPrepareModel768
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel768, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6700
 * @tc.name   : testPrepareModel769
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel769, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6710
 * @tc.name   : testPrepareModel770
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel770, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6720
 * @tc.name   : testPrepareModel771
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel771, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6730
 * @tc.name   : testPrepareModel772
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel772, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6740
 * @tc.name   : testPrepareModel773
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel773, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6750
 * @tc.name   : testPrepareModel774
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel774, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6760
 * @tc.name   : testPrepareModel775
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel775, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6770
 * @tc.name   : testPrepareModel776
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel776, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6780
 * @tc.name   : testPrepareModel777
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel777, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6790
 * @tc.name   : testPrepareModel778
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel778, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6800
 * @tc.name   : testPrepareModel779
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel779, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6810
 * @tc.name   : testPrepareModel780
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel780, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6820
 * @tc.name   : testPrepareModel781
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel781, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6830
 * @tc.name   : testPrepareModel782
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel782, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6840
 * @tc.name   : testPrepareModel783
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel783, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6850
 * @tc.name   : testPrepareModel784
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel784, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6860
 * @tc.name   : testPrepareModel785
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel785, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6870
 * @tc.name   : testPrepareModel786
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel786, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6880
 * @tc.name   : testPrepareModel787
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel787, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6890
 * @tc.name   : testPrepareModel788
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel788, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6900
 * @tc.name   : testPrepareModel789
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel789, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6910
 * @tc.name   : testPrepareModel790
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel790, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6920
 * @tc.name   : testPrepareModel791
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel791, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6930
 * @tc.name   : testPrepareModel792
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel792, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6940
 * @tc.name   : testPrepareModel793
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel793, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6950
 * @tc.name   : testPrepareModel794
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel794, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6960
 * @tc.name   : testPrepareModel795
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel795, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6970
 * @tc.name   : testPrepareModel796
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel796, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_INT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6980
 * @tc.name   : testPrepareModel797
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel797, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_6990
 * @tc.name   : testPrepareModel798
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel798, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7000
 * @tc.name   : testPrepareModel799
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel799, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7010
 * @tc.name   : testPrepareModel800
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel800, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7020
 * @tc.name   : testPrepareModel801
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel801, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7030
 * @tc.name   : testPrepareModel802
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel802, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7040
 * @tc.name   : testPrepareModel803
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel803, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7050
 * @tc.name   : testPrepareModel804
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel804, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7060
 * @tc.name   : testPrepareModel805
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel805, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7070
 * @tc.name   : testPrepareModel806
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel806, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7080
 * @tc.name   : testPrepareModel807
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel807, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7090
 * @tc.name   : testPrepareModel808
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel808, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7100
 * @tc.name   : testPrepareModel809
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel809, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7110
 * @tc.name   : testPrepareModel810
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel810, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7120
 * @tc.name   : testPrepareModel811
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel811, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7130
 * @tc.name   : testPrepareModel812
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel812, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7140
 * @tc.name   : testPrepareModel813
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel813, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7150
 * @tc.name   : testPrepareModel814
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel814, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7160
 * @tc.name   : testPrepareModel815
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel815, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7170
 * @tc.name   : testPrepareModel816
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel816, Function | MediumTest | Level2)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT8;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_INVALID_MODEL, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7180
 * @tc.name   : testPrepareModel817
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel817, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7190
 * @tc.name   : testPrepareModel818
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel818, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7200
 * @tc.name   : testPrepareModel819
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel819, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7210
 * @tc.name   : testPrepareModel820
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel820, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7220
 * @tc.name   : testPrepareModel821
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel821, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7230
 * @tc.name   : testPrepareModel822
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel822, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7240
 * @tc.name   : testPrepareModel823
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel823, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7250
 * @tc.name   : testPrepareModel824
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel824, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7260
 * @tc.name   : testPrepareModel825
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel825, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7270
 * @tc.name   : testPrepareModel826
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel826, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7280
 * @tc.name   : testPrepareModel827
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel827, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7290
 * @tc.name   : testPrepareModel828
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel828, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7300
 * @tc.name   : testPrepareModel829
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel829, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7310
 * @tc.name   : testPrepareModel830
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel830, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7320
 * @tc.name   : testPrepareModel831
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel831, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7330
 * @tc.name   : testPrepareModel832
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel832, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7340
 * @tc.name   : testPrepareModel833
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel833, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7350
 * @tc.name   : testPrepareModel834
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel834, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7360
 * @tc.name   : testPrepareModel835
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel835, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7370
 * @tc.name   : testPrepareModel836
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel836, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7380
 * @tc.name   : testPrepareModel837
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel837, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7390
 * @tc.name   : testPrepareModel838
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel838, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7400
 * @tc.name   : testPrepareModel839
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel839, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7410
 * @tc.name   : testPrepareModel840
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel840, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7420
 * @tc.name   : testPrepareModel841
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel841, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7430
 * @tc.name   : testPrepareModel842
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel842, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7440
 * @tc.name   : testPrepareModel843
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel843, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7450
 * @tc.name   : testPrepareModel844
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel844, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7460
 * @tc.name   : testPrepareModel845
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel845, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7470
 * @tc.name   : testPrepareModel846
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel846, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7480
 * @tc.name   : testPrepareModel847
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel847, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7490
 * @tc.name   : testPrepareModel848
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel848, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7500
 * @tc.name   : testPrepareModel849
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel849, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7510
 * @tc.name   : testPrepareModel850
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel850, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7520
 * @tc.name   : testPrepareModel851
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel851, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7530
 * @tc.name   : testPrepareModel852
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel852, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7540
 * @tc.name   : testPrepareModel853
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel853, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7550
 * @tc.name   : testPrepareModel854
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel854, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7560
 * @tc.name   : testPrepareModel855
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel855, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7570
 * @tc.name   : testPrepareModel856
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel856, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7580
 * @tc.name   : testPrepareModel857
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel857, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7590
 * @tc.name   : testPrepareModel858
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel858, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7600
 * @tc.name   : testPrepareModel859
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel859, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7610
 * @tc.name   : testPrepareModel860
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel860, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7620
 * @tc.name   : testPrepareModel861
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel861, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7630
 * @tc.name   : testPrepareModel862
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel862, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7640
 * @tc.name   : testPrepareModel863
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel863, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7650
 * @tc.name   : testPrepareModel864
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel864, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7660
 * @tc.name   : testPrepareModel865
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel865, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7670
 * @tc.name   : testPrepareModel866
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel866, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7680
 * @tc.name   : testPrepareModel867
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel867, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7690
 * @tc.name   : testPrepareModel868
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel868, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7700
 * @tc.name   : testPrepareModel869
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel869, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7710
 * @tc.name   : testPrepareModel870
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel870, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7720
 * @tc.name   : testPrepareModel871
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel871, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7730
 * @tc.name   : testPrepareModel872
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel872, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7740
 * @tc.name   : testPrepareModel873
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel873, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7750
 * @tc.name   : testPrepareModel874
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel874, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7760
 * @tc.name   : testPrepareModel875
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel875, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7770
 * @tc.name   : testPrepareModel876
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel876, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_UINT64;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7780
 * @tc.name   : testPrepareModel877
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel877, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7790
 * @tc.name   : testPrepareModel878
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel878, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7800
 * @tc.name   : testPrepareModel879
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel879, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7810
 * @tc.name   : testPrepareModel880
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel880, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7820
 * @tc.name   : testPrepareModel881
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel881, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7830
 * @tc.name   : testPrepareModel882
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel882, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7840
 * @tc.name   : testPrepareModel883
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel883, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7850
 * @tc.name   : testPrepareModel884
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel884, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7860
 * @tc.name   : testPrepareModel885
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel885, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7870
 * @tc.name   : testPrepareModel886
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel886, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7880
 * @tc.name   : testPrepareModel887
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel887, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7890
 * @tc.name   : testPrepareModel888
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel888, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7900
 * @tc.name   : testPrepareModel889
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel889, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7910
 * @tc.name   : testPrepareModel890
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel890, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7920
 * @tc.name   : testPrepareModel891
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel891, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7930
 * @tc.name   : testPrepareModel892
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel892, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7940
 * @tc.name   : testPrepareModel893
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel893, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7950
 * @tc.name   : testPrepareModel894
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel894, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7960
 * @tc.name   : testPrepareModel895
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel895, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7970
 * @tc.name   : testPrepareModel896
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel896, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT16;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7980
 * @tc.name   : testPrepareModel897
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel897, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_7990
 * @tc.name   : testPrepareModel898
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel898, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8000
 * @tc.name   : testPrepareModel899
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel899, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8010
 * @tc.name   : testPrepareModel900
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel900, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_NONE, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8020
 * @tc.name   : testPrepareModel901
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel901, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8030
 * @tc.name   : testPrepareModel902
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel902, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8040
 * @tc.name   : testPrepareModel903
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel903, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8050
 * @tc.name   : testPrepareModel904
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel904, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_LOW, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8060
 * @tc.name   : testPrepareModel905
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel905, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8070
 * @tc.name   : testPrepareModel906
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel906, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8080
 * @tc.name   : testPrepareModel907
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel907, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8090
 * @tc.name   : testPrepareModel908
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel908, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_MEDIUM, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8100
 * @tc.name   : testPrepareModel909
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel909, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8110
 * @tc.name   : testPrepareModel910
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel910, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8120
 * @tc.name   : testPrepareModel911
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel911, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8130
 * @tc.name   : testPrepareModel912
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel912, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_HIGH, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8140
 * @tc.name   : testPrepareModel913
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel913, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_NONE};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8150
 * @tc.name   : testPrepareModel914
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel914, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_LOW};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8160
 * @tc.name   : testPrepareModel915
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel915, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_MEDIUM};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Modelcompile_PrepareModelV2_8170
 * @tc.name   : testPrepareModel916
 * @tc.desc   : test parameter allTensors dataType
 */
HWTEST_F(ModelPrepareTestAdditional, testPrepareModel916, Function | MediumTest | Level1)
{
    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    auto &Tensor = iModel->allTensors[0];
    Tensor.dataType = V2_0::DATA_TYPE_FLOAT32;
    V2_0::ModelConfig modelConfig{true, V2_0::PERFORMANCE_EXTREME, V2_0::PRIORITY_HIGH};
    V2_0::sptr<V2_0::IPreparedModel> preparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, modelConfig, preparedModel));

    mindspore::lite::MindIR_Model_Destroy(&iModel);
    if (tensorBuffer.fd != -1) {
        EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->ReleaseBuffer(tensorBuffer));
    }
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0100
 * @tc.name   : testV2PrepareModelFromModelCache001
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_NONE; config.priority =
 * V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache001, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0200
 * @tc.name   : testV2PrepareModelFromModelCache002
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_NONE; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache002, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0300
 * @tc.name   : testV2PrepareModelFromModelCache003
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_NONE; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache003, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0400
 * @tc.name   : testV2PrepareModelFromModelCache004
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_LOW; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache004, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0500
 * @tc.name   : testV2PrepareModelFromModelCache005
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *               config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_LOW; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache005, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0600
 * @tc.name   : testV2PrepareModelFromModelCache006
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_LOW; config.priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache006, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0700
 * @tc.name   : testV2PrepareModelFromModelCache007
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_LOW; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache007, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0800
 * @tc.name   : testV2PrepareModelFromModelCache008
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache008, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_0900
 * @tc.name   : testV2PrepareModelFromModelCache009
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache009, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1000
 * @tc.name   : testV2PrepareModelFromModelCache010
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache010, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1100
 * @tc.name   : testV2PrepareModelFromModelCache011
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache011, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1200
 * @tc.name   : testV2PrepareModelFromModelCache012
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_HIGH; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache012, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1300
 * @tc.name   : testV2PrepareModelFromModelCache013
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_HIGH; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache013, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1400
 * @tc.name   : testV2PrepareModelFromModelCache014
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_HIGH; config.priority =
 * V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache014, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1500
 * @tc.name   : testV2PrepareModelFromModelCache015
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_HIGH; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache015, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1600
 * @tc.name   : testV2PrepareModelFromModelCache016
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache016, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1700
 * @tc.name   : testV2PrepareModelFromModelCache017
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache017, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1800
 * @tc.name   : testV2PrepareModelFromModelCache018
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache018, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_1900
 * @tc.name   : testV2PrepareModelFromModelCache019
 * @tc.desc   : Testing the effectiveness of the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache019, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2000
 * @tc.name   : testV2PrepareModelFromModelCache020
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_NONE; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache020, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2100
 * @tc.name   : testV2PrepareModelFromModelCache021
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_NONE; config.priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache021, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2200
 * @tc.name   : testV2PrepareModelFromModelCache022
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_NONE; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache022, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2300
 * @tc.name   : testV2PrepareModelFromModelCache023
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_NONE; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache023, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2400
 * @tc.name   : testV2PrepareModelFromModelCache024
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_LOW; config.priority = V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache024, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2500
 * @tc.name   : testV2PrepareModelFromModelCache025
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_LOW; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache025, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2600
 * @tc.name   : testV2PrepareModelFromModelCache026
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_LOW; config.priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache026, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2700
 * @tc.name   : testV2PrepareModelFromModelCache027
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_LOW; config.priority = V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache027, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_LOW;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2800
 * @tc.name   : testV2PrepareModelFromModelCache028
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache028, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_2900
 * @tc.name   : testV2PrepareModelFromModelCache029
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache029, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3000
 * @tc.name   : testV2PrepareModelFromModelCache030
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache030, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3100
 * @tc.name   : testV2PrepareModelFromModelCache031
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_MEDIUM; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache031, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_MEDIUM;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3200
 * @tc.name   : testV2PrepareModelFromModelCache032
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_HIGH; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache032, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3300
 * @tc.name   : testV2PrepareModelFromModelCache033
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_HIGH; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache033, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3400
 * @tc.name   : testV2PrepareModelFromModelCache034
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_HIGH; config.priority = V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache034, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3500
 * @tc.name   : testV2PrepareModelFromModelCache035
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_HIGH; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache035, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_HIGH;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3600
 * @tc.name   : testV2PrepareModelFromModelCache036
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache036, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3700
 * @tc.name   : testV2PrepareModelFromModelCache037
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_LOW;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache037, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_LOW;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3800
 * @tc.name   : testV2PrepareModelFromModelCache038
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_MEDIUM;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache038, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_MEDIUM;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_3900
 * @tc.name   : testV2PrepareModelFromModelCache039
 * @tc.desc   : Testing the PrepareModelFromModelCache function
 *              config.enableFloat16 = true; config.mode = V2_0::PERFORMANCE_EXTREME; config.priority =
 * V2_0::PRIORITY_HIGH;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache039, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = true;
    config.mode = V2_0::PERFORMANCE_EXTREME;
    config.priority = V2_0::PRIORITY_HIGH;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}

/**
 * @tc.number : SUB_AI_Nnrt_Func_HDI_Device_PrepareModelFromModelCache_4000
 * @tc.name   : testV2PrepareModelFromModelCache040
 * @tc.desc   : Testing  the PrepareModelFromModelCache function
 *              config.enableFloat16 = false; config.mode = V2_0::PERFORMANCE_NONE; config.priority =
 * V2_0::PRIORITY_NONE;
 */
HWTEST_F(ModelPrepareTestAdditional, testV2PrepareModelFromModelCache040, Function | MediumTest | Level1)
{
    bool isSupportedCache = false;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->IsModelCacheSupported(isSupportedCache));
    if (!isSupportedCache) {
        GTEST_SKIP() << "Export cache is not supported.";
    }

    OH_NNModel *model = nullptr;
    HDICommon::BuildAddGraph(&model);
    ASSERT_NE(model, nullptr);

    V2_0::Model *iModel = nullptr;
    V2_0::SharedBuffer tensorBuffer{NNRT_INVALID_FD, 0, 0, 0};
    ASSERT_EQ(OH_NN_SUCCESS, HDICommon::ConvertModel(device_, model, tensorBuffer, &iModel));

    V2_0::ModelConfig config;
    config.enableFloat16 = false;
    config.mode = V2_0::PERFORMANCE_NONE;
    config.priority = V2_0::PRIORITY_NONE;
    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, device_->PrepareModel(*iModel, config, iPreparedModel));

    std::vector<V2_0::SharedBuffer> modelCache;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS, iPreparedModel->ExportModelCache(modelCache));

    OHOS::sptr<V2_0::IPreparedModel> iPreparedModel1;
    EXPECT_EQ(V2_0::NNRT_ReturnCode::NNRT_SUCCESS,
              device_->PrepareModelFromModelCache(modelCache, config, iPreparedModel1));
}